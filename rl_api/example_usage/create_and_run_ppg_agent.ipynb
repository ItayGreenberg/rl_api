{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from rl_api.environments.toy_env import ToyVectorEnv\n",
    "from rl_api.environments.wrappers import AutoResetVectorEnv\n",
    "from rl_api.environments.types import BatchObs\n",
    "\n",
    "from rl_api.agents.ppg_utils.configs import (\n",
    "    DimensionConfig,\n",
    "    PPOConfig,\n",
    "    PPGConfig,\n",
    "    BufferConfig,\n",
    "    EntropySchedulerConfig,\n",
    "    TrainingConfig,\n",
    "    EvalConfig,\n",
    "    LoggingConfig,\n",
    "    SavingConfig,\n",
    ")\n",
    "from rl_api.agents.ppg_utils.buffer import RolloutBufferVec\n",
    "from rl_api.agents.ppg_utils.vectorized_agent import VectorizedPPGAgent\n",
    "from rl_api.agents.ppg_utils.factories import build_ppg_agent\n",
    "\n",
    "from rl_api.networks.networks import ActorNetwork, CriticNetwork, PPGPolicyNetwork, PPGValueNetwork\n",
    "from rl_api.networks.networks_factory import build_ppg_optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "agent_dir_path = \"./toy_ppg_agent\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- simple encoder for (seq_len, 1) obs ---------\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        in_dim = int(np.prod(obs_shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),            # (B, *obs_shape) -> (B, in_dim)\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, *obs_shape)\n",
    "        return self.net(x)  # (B, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- envs ----------\n",
    "n_envs = 16\n",
    "seq_len = 8\n",
    "feature_dim = 1\n",
    "obs_shape = (seq_len, feature_dim)\n",
    "context_dim = 1\n",
    "action_dim = 3\n",
    "\n",
    "base_train_env = ToyVectorEnv(\n",
    "    n_envs=n_envs,\n",
    "    seq_len=seq_len,\n",
    "    step_size=0.1,\n",
    "    max_steps=50,\n",
    "    x_limit=2.0,\n",
    "    device=device,\n",
    ")\n",
    "train_env = AutoResetVectorEnv(base_train_env)\n",
    "\n",
    "# For simplicity, use the same env for eval (could be separate)\n",
    "eval_env = AutoResetVectorEnv(\n",
    "    ToyVectorEnv(\n",
    "        n_envs=n_envs,\n",
    "        seq_len=seq_len,\n",
    "        step_size=0.1,\n",
    "        max_steps=50,\n",
    "        x_limit=2.0,\n",
    "        device=device,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- configs ----------\n",
    "dims = DimensionConfig(\n",
    "    obs_shape=obs_shape,\n",
    "    action_dim=action_dim,\n",
    "    context_dim=context_dim,\n",
    ")\n",
    "\n",
    "ppo_cfg = PPOConfig(\n",
    "    clip_eps=0.1,\n",
    "    entropy_coef=0.01,\n",
    "    clip_vf=None,\n",
    "    vf_coef=0.5,\n",
    "    vf_loss_clip=False,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    target_kl=None,\n",
    "    grad_clip=0.5,\n",
    ")\n",
    "\n",
    "ppg_cfg = PPGConfig(\n",
    "    n_pi=8,\n",
    "    policy_epochs=1,\n",
    "    critic_epochs=1,\n",
    "    aux_epochs=3,\n",
    "    beta_kl=0.01,\n",
    ")\n",
    "\n",
    "buf_cfg = BufferConfig(\n",
    "    buffer_size=1024,\n",
    "    ppo_batch_size=64,\n",
    "    aux_batch_size=64,\n",
    ")\n",
    "\n",
    "entropy_sched_cfg = EntropySchedulerConfig(\n",
    "    use_scheduler=False\n",
    ")\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    total_updates=50,  # small number just for the demo\n",
    ")\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    eval_method=\"sample\",\n",
    "    n_steps=256\n",
    ")\n",
    "os.makedirs(agent_dir_path, exist_ok=True)\n",
    "\n",
    "logging_cfg = LoggingConfig(\n",
    "    current_update=0,\n",
    "    log_interval=2,\n",
    "    eval_interval=10,\n",
    "    html_log_path=os.path.join(agent_dir_path, \"html_logs\"),          \n",
    "    tensorboard_path=os.path.join(agent_dir_path, \"tensorboard_logs\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "saving_cfg = SavingConfig(\n",
    "    save_interval=25,\n",
    "    save_agent_path=agent_dir_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- networks ----------\n",
    "policy_encoder = MLPEncoder(obs_shape=obs_shape, hidden_dim=32).to(device)\n",
    "value_encoder = MLPEncoder(obs_shape=obs_shape, hidden_dim=32).to(device)\n",
    "\n",
    "actor_net = ActorNetwork(\n",
    "    obs_embed_dim=policy_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "aux_net = CriticNetwork(\n",
    "    obs_embed_dim=policy_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "value_net = CriticNetwork(\n",
    "    obs_embed_dim=value_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "policy_net = PPGPolicyNetwork(\n",
    "    encoder=policy_encoder,\n",
    "    action_head=actor_net,\n",
    "    aux_value_head=aux_net,\n",
    ").to(device)\n",
    "\n",
    "value_net = PPGValueNetwork(\n",
    "    encoder=value_encoder,\n",
    "    value_head=value_net,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizers = build_ppg_optimizers(\n",
    "    policy_network=policy_net,\n",
    "    value_network=value_net,\n",
    "    enc_lr=1e-4,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    weight_decay=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20837c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- agent ----------\n",
    "agent: VectorizedPPGAgent = build_ppg_agent(\n",
    "    policy_net=policy_net,\n",
    "    value_net=value_net,\n",
    "    policy_optimizer=optimizers[\"policy_optimizer\"],\n",
    "    aux_optimizer=optimizers[\"aux_optimizer\"],\n",
    "    critic_optimizer=optimizers[\"critic_optimizer\"],\n",
    "    train_env=train_env,\n",
    "    eval_env=eval_env,\n",
    "    dims=dims,\n",
    "    ppo_cfg=ppo_cfg,\n",
    "    ppg_cfg=ppg_cfg,\n",
    "    buf_cfg=buf_cfg,\n",
    "    entropy_sched_cfg=entropy_sched_cfg,\n",
    "    eval_cfg=eval_cfg,\n",
    "    logging_cfg=logging_cfg,\n",
    "    saving_cfg=saving_cfg,\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb4958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Upd   2 | step: 0]\n",
      "| pi_loss: -0.0006  v_loss: +0.5077  ent: 1.097  ent_coef: 0.0100  kl: -0.0000  batches_used: 100.000%  expl_var: 0.00  clip: 0.00%\n",
      "| adv_mean: -5.5379  adv_std: 4.5308  rets_mean: -5.3936  rets_std: 4.5308\n",
      "| reward_sum: -14.684  reward_pre_step: -0.45887\n",
      "[Grads] policy_enc: 0.115  act: 0.159  value_enc: 0.132, critic: 0.254\n",
      "\n",
      "[Upd   4 | step: 0]\n",
      "| pi_loss: -0.0005  v_loss: +0.4906  ent: 1.097  ent_coef: 0.0100  kl: 0.0001  batches_used: 100.000%  expl_var: 0.03  clip: 0.00%\n",
      "| adv_mean: -5.4116  adv_std: 5.0492  rets_mean: -5.2867  rets_std: 5.0634\n",
      "| reward_sum: -16.256  reward_pre_step: -0.50799\n",
      "[Grads] policy_enc: 0.0959  act: 0.163  value_enc: 0.132, critic: 0.273\n",
      "\n",
      "[Upd   6 | step: 0]\n",
      "| pi_loss: -0.0006  v_loss: +0.4772  ent: 1.096  ent_coef: 0.0100  kl: 0.0002  batches_used: 100.000%  expl_var: 0.06  clip: 0.00%\n",
      "| adv_mean: -5.7060  adv_std: 4.6195  rets_mean: -5.5984  rets_std: 4.6448\n",
      "| reward_sum: -16.786  reward_pre_step: -0.52457\n",
      "[Grads] policy_enc: 0.101  act: 0.164  value_enc: 0.171, critic: 0.3\n",
      "\n",
      "[Upd   8 | step: 0]\n",
      "| pi_loss: -0.0005  v_loss: +0.4534  ent: 1.096  ent_coef: 0.0100  kl: -0.0001  batches_used: 100.000%  expl_var: 0.10  clip: 0.00%\n",
      "| adv_mean: -4.8688  adv_std: 3.9491  rets_mean: -4.7750  rets_std: 3.9970\n",
      "| reward_sum: -14.342  reward_pre_step: -0.44818\n",
      "[Grads] policy_enc: 0.0946  act: 0.167  value_enc: 0.197, critic: 0.316\n",
      "\n",
      "[Upd  10 | step: 0]\n",
      "| pi_loss: -0.0006  v_loss: +0.5117  ent: 1.096  ent_coef: 0.0100  kl: 0.0001  batches_used: 100.000%  expl_var: 0.11  clip: 0.00%\n",
      "| adv_mean: -5.2379  adv_std: 4.4121  rets_mean: -5.6243  rets_std: 4.4630\n",
      "| reward_sum: -16.283  reward_pre_step: -0.50884\n",
      "[Grads] policy_enc: 0.13  act: 0.182  value_enc: 0.383, critic: 0.317\n",
      "[Eval @ update 10] New best agent found! sum reward: -139.17, reward_per_step: -0.5436\n",
      "\n",
      "[Upd  12 | step: 0]\n",
      "| pi_loss: -0.0006  v_loss: +0.4629  ent: 1.096  ent_coef: 0.0100  kl: 0.0001  batches_used: 100.000%  expl_var: 0.12  clip: 0.00%\n",
      "| adv_mean: -5.0252  adv_std: 3.9889  rets_mean: -5.2678  rets_std: 4.0512\n",
      "| reward_sum: -13.824  reward_pre_step: -0.43199\n",
      "[Grads] policy_enc: 0.12  act: 0.208  value_enc: 0.384, critic: 0.31\n",
      "\n",
      "[Upd  14 | step: 0]\n",
      "| pi_loss: -0.0005  v_loss: +0.4018  ent: 1.095  ent_coef: 0.0100  kl: -0.0003  batches_used: 100.000%  expl_var: 0.20  clip: 0.00%\n",
      "| adv_mean: -3.6539  adv_std: 3.0538  rets_mean: -3.6909  rets_std: 3.1536\n",
      "| reward_sum: -11.744  reward_pre_step: -0.36701\n",
      "[Grads] policy_enc: 0.113  act: 0.191  value_enc: 0.356, critic: 0.339\n",
      "\n",
      "[Upd  16 | step: 0]\n",
      "| pi_loss: -0.0004  v_loss: +0.3489  ent: 1.093  ent_coef: 0.0100  kl: 0.0000  batches_used: 100.000%  expl_var: 0.31  clip: 0.00%\n",
      "| adv_mean: -5.1498  adv_std: 4.4896  rets_mean: -5.2556  rets_std: 4.6672\n",
      "| reward_sum: -14.744  reward_pre_step: -0.46075\n",
      "[Grads] policy_enc: 0.118  act: 0.168  value_enc: 0.373, critic: 0.325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(train_cfg\u001b[38;5;241m=\u001b[39mtrain_cfg)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppg_utils/vectorized_agent.py:677\u001b[0m, in \u001b[0;36mVectorizedPPGAgent.train\u001b[0;34m(self, train_cfg)\u001b[0m\n\u001b[1;32m    675\u001b[0m initial_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m initial_update \u001b[38;5;241m+\u001b[39m total_updates:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_one_update(update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppg_utils/vectorized_agent.py:692\u001b[0m, in \u001b[0;36mVectorizedPPGAgent.train_one_update\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# 1) Collect rollout & compute returns\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs) \n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# 2) PPO policy phase        \u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_phase_update()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppg_utils/vectorized_agent.py:343\u001b[0m, in \u001b[0;36mVectorizedPPGAgent.collect_rollouts\u001b[0;34m(self, vec_env, obs)\u001b[0m\n\u001b[1;32m    337\u001b[0m next_obs, rewards, dones \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(actions_np)  \u001b[38;5;66;03m# likely numpy outputs\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# -------------- store in buffer -------------------\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# buffer will move things to its own device; we keep inputs on CPU here\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_buf\u001b[38;5;241m.\u001b[39madd_batch(\n\u001b[1;32m    344\u001b[0m     obs         \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mobs,            \u001b[38;5;66;03m# (n_envs, *obs_shape), torch\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     ctx         \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mctx,            \u001b[38;5;66;03m# (n_envs, *ctx_shape), torch\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     action_mask \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39maction_mask,    \u001b[38;5;66;03m# (n_envs, n_actions), torch\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     actions     \u001b[38;5;241m=\u001b[39m actions_cpu,        \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     logps       \u001b[38;5;241m=\u001b[39m logps\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),   \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     logits      \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m    350\u001b[0m     values      \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),  \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     rewards     \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     dones       \u001b[38;5;241m=\u001b[39m dones\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),   \u001b[38;5;66;03m# (n_envs,), bool torch\u001b[39;00m\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m    356\u001b[0m last_dones \u001b[38;5;241m=\u001b[39m dones\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()    \u001b[38;5;66;03m# keep last dones for bootstrap masking\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppg_utils/buffer.py:154\u001b[0m, in \u001b[0;36mRolloutBufferVec.add_batch\u001b[0;34m(self, obs, ctx, action_mask, actions, logps, values, rewards, dones, logits)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n_envs, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits first dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != num_envs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    155\u001b[0m         obs\u001b[38;5;241m=\u001b[39mobs[env_i],\n\u001b[1;32m    156\u001b[0m         context\u001b[38;5;241m=\u001b[39mctx[env_i],\n\u001b[1;32m    157\u001b[0m         action\u001b[38;5;241m=\u001b[39mactions[env_i],\n\u001b[1;32m    158\u001b[0m         logp\u001b[38;5;241m=\u001b[39mlogps[env_i],\n\u001b[1;32m    159\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalues[env_i],\n\u001b[1;32m    160\u001b[0m         reward\u001b[38;5;241m=\u001b[39mrewards[env_i],\n\u001b[1;32m    161\u001b[0m         done\u001b[38;5;241m=\u001b[39mdones[env_i],\n\u001b[1;32m    162\u001b[0m         action_mask\u001b[38;5;241m=\u001b[39maction_mask[env_i],\n\u001b[1;32m    163\u001b[0m         env_id\u001b[38;5;241m=\u001b[39menv_i,\n\u001b[1;32m    164\u001b[0m         logits\u001b[38;5;241m=\u001b[39m(logits[env_i] \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    165\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(train_cfg=train_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_trading_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
