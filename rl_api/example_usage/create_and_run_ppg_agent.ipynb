{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from rl_api.environments.toy_env import ToyVectorEnv\n",
    "from rl_api.environments.wrappers import AutoResetVectorEnv\n",
    "from rl_api.environments.types import BatchObs\n",
    "\n",
    "from rl_api.agents.ppg_utils.configs import (\n",
    "    DimensionConfig,\n",
    "    PPOConfig,\n",
    "    PPGConfig,\n",
    "    BufferConfig,\n",
    "    EntropySchedulerConfig,\n",
    "    TrainingConfig,\n",
    "    EvalConfig,\n",
    "    LoggingConfig,\n",
    "    SavingConfig,\n",
    ")\n",
    "from rl_api.agents.ppg_utils.buffer import RolloutBufferVec\n",
    "from rl_api.agents.ppg_utils.vectorized_agent import VectorizedPPGAgent\n",
    "from rl_api.agents.ppg_utils.factories import build_ppg_agent\n",
    "\n",
    "from rl_api.networks.networks import ActorNetwork, CriticNetwork, PPGPolicyNetwork, PPGValueNetwork\n",
    "from rl_api.networks.networks_factory import build_ppg_optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "agent_dir_path = \"./toy_ppg_agent\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- simple encoder for (seq_len, 1) obs ---------\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        in_dim = int(np.prod(obs_shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),            # (B, *obs_shape) -> (B, in_dim)\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, *obs_shape)\n",
    "        return self.net(x)  # (B, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- envs ----------\n",
    "n_envs = 16\n",
    "seq_len = 8\n",
    "feature_dim = 1\n",
    "obs_shape = (seq_len, feature_dim)\n",
    "context_dim = 1\n",
    "action_dim = 3\n",
    "\n",
    "base_train_env = ToyVectorEnv(\n",
    "    n_envs=n_envs,\n",
    "    seq_len=seq_len,\n",
    "    step_size=0.1,\n",
    "    max_steps=50,\n",
    "    x_limit=2.0,\n",
    "    device=device,\n",
    ")\n",
    "train_env = AutoResetVectorEnv(base_train_env)\n",
    "\n",
    "# For simplicity, use the same env for eval (could be separate)\n",
    "eval_env = AutoResetVectorEnv(\n",
    "    ToyVectorEnv(\n",
    "        n_envs=n_envs,\n",
    "        seq_len=seq_len,\n",
    "        step_size=0.1,\n",
    "        max_steps=50,\n",
    "        x_limit=2.0,\n",
    "        device=device,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- configs ----------\n",
    "dims = DimensionConfig(\n",
    "    obs_shape=obs_shape,\n",
    "    action_dim=action_dim,\n",
    "    context_dim=context_dim,\n",
    ")\n",
    "\n",
    "ppo_cfg = PPOConfig(\n",
    "    clip_eps=0.1,\n",
    "    entropy_coef=0.01,\n",
    "    clip_vf=None,\n",
    "    vf_coef=0.5,\n",
    "    vf_loss_clip=False,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    target_kl=None,\n",
    "    grad_clip=0.5,\n",
    ")\n",
    "\n",
    "ppg_cfg = PPGConfig(\n",
    "    n_pi=8,\n",
    "    policy_epochs=1,\n",
    "    critic_epochs=1,\n",
    "    aux_epochs=3,\n",
    "    beta_kl=0.01,\n",
    ")\n",
    "\n",
    "buf_cfg = BufferConfig(\n",
    "    buffer_size=1024,\n",
    "    ppo_batch_size=64,\n",
    "    aux_batch_size=64,\n",
    ")\n",
    "\n",
    "entropy_sched_cfg = EntropySchedulerConfig(\n",
    "    use_scheduler=False\n",
    ")\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    total_updates=50,  # small number just for the demo\n",
    ")\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    eval_method=\"sample\",\n",
    "    n_steps=256\n",
    ")\n",
    "os.makedirs(agent_dir_path, exist_ok=True)\n",
    "\n",
    "logging_cfg = LoggingConfig(\n",
    "    current_update=0,\n",
    "    log_interval=2,\n",
    "    eval_interval=10,\n",
    "    html_log_path=os.path.join(agent_dir_path, \"html_logs\"),          \n",
    "    tensorboard_path=os.path.join(agent_dir_path, \"tensorboard_logs\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "saving_cfg = SavingConfig(\n",
    "    save_interval=25,\n",
    "    save_agent_path=agent_dir_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- networks ----------\n",
    "policy_encoder = MLPEncoder(obs_shape=obs_shape, hidden_dim=32).to(device)\n",
    "value_encoder = MLPEncoder(obs_shape=obs_shape, hidden_dim=32).to(device)\n",
    "\n",
    "actor_net = ActorNetwork(\n",
    "    obs_embed_dim=policy_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "aux_net = CriticNetwork(\n",
    "    obs_embed_dim=policy_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "value_net = CriticNetwork(\n",
    "    obs_embed_dim=value_encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    hidden_units=[32, 16], # must start with hidden_dim of the encoder\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "policy_net = PPGPolicyNetwork(\n",
    "    encoder=policy_encoder,\n",
    "    action_head=actor_net,\n",
    "    aux_value_head=aux_net,\n",
    ").to(device)\n",
    "\n",
    "value_net = PPGValueNetwork(\n",
    "    encoder=value_encoder,\n",
    "    value_head=value_net,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizers = build_ppg_optimizers(\n",
    "    policy_network=policy_net,\n",
    "    value_network=value_net,\n",
    "    enc_lr=1e-4,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    weight_decay=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20837c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- agent ----------\n",
    "agent: VectorizedPPGAgent = build_ppg_agent(\n",
    "    policy_net=policy_net,\n",
    "    value_net=value_net,\n",
    "    policy_optimizer=optimizers[\"policy_optimizer\"],\n",
    "    aux_optimizer=optimizers[\"aux_optimizer\"],\n",
    "    critic_optimizer=optimizers[\"critic_optimizer\"],\n",
    "    train_env=train_env,\n",
    "    eval_env=eval_env,\n",
    "    dims=dims,\n",
    "    ppo_cfg=ppo_cfg,\n",
    "    ppg_cfg=ppg_cfg,\n",
    "    buf_cfg=buf_cfg,\n",
    "    entropy_sched_cfg=entropy_sched_cfg,\n",
    "    eval_cfg=eval_cfg,\n",
    "    logging_cfg=logging_cfg,\n",
    "    saving_cfg=saving_cfg,\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(train_cfg=train_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_trading_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
