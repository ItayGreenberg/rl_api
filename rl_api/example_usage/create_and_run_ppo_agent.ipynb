{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255d684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 14:57:35.285548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765717055.378889    2009 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765717055.404869    2009 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-14 14:57:35.608907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from rl_api.environments.toy_env import ToyVectorEnv\n",
    "from rl_api.environments.wrappers import AutoResetVectorEnv\n",
    "from rl_api.environments.types import BatchObs\n",
    "\n",
    "from rl_api.agents.ppo_utils.configs import (\n",
    "    DimensionConfig,\n",
    "    PPOConfig,\n",
    "    BufferConfig,\n",
    "    EntropySchedulerConfig,\n",
    "    TrainingConfig,\n",
    "    EvalConfig,\n",
    "    LoggingConfig,\n",
    "    SavingConfig,\n",
    ")\n",
    "from rl_api.agents.ppo_utils.buffer import RolloutBufferVec\n",
    "from rl_api.agents.ppo_utils.vectorized_agent import VectorizedPPOAgent\n",
    "from rl_api.agents.ppo_utils.factories import build_ppo_agent\n",
    "\n",
    "from rl_api.networks.networks import ActorNetwork, CriticNetwork, PPOPolicyNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e98b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "agent_dir_path = \"./toy_ppo_agent\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6503b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- simple encoder for (seq_len, 1) obs ---------\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        in_dim = int(np.prod(obs_shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),            # (B, *obs_shape) -> (B, in_dim)\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, *obs_shape)\n",
    "        return self.net(x)  # (B, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ad1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- envs ----------\n",
    "n_envs = 16\n",
    "seq_len = 8\n",
    "feature_dim = 1\n",
    "obs_shape = (seq_len, feature_dim)\n",
    "context_dim = 1\n",
    "action_dim = 3\n",
    "\n",
    "base_train_env = ToyVectorEnv(\n",
    "    n_envs=n_envs,\n",
    "    seq_len=seq_len,\n",
    "    step_size=0.1,\n",
    "    max_steps=50,\n",
    "    x_limit=2.0,\n",
    "    device=device,\n",
    ")\n",
    "train_env = AutoResetVectorEnv(base_train_env)\n",
    "\n",
    "# For simplicity, use the same env for eval (could be separate)\n",
    "eval_env = AutoResetVectorEnv(\n",
    "    ToyVectorEnv(\n",
    "        n_envs=n_envs,\n",
    "        seq_len=seq_len,\n",
    "        step_size=0.1,\n",
    "        max_steps=50,\n",
    "        x_limit=2.0,\n",
    "        device=device,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- configs ----------\n",
    "dims = DimensionConfig(\n",
    "    obs_shape=obs_shape,\n",
    "    action_dim=action_dim,\n",
    "    context_dim=context_dim,\n",
    ")\n",
    "\n",
    "ppo_cfg = PPOConfig(\n",
    "    num_epochs=2,\n",
    "    clip_eps=0.2,\n",
    "    entropy_coef=0.01,\n",
    "    clip_vf=None,\n",
    "    vf_coef=0.5,\n",
    "    vf_loss_clip=False,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    target_kl=None,\n",
    "    grad_clip=0.5,\n",
    ")\n",
    "\n",
    "buf_cfg = BufferConfig(\n",
    "    buffer_size=1024,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "entropy_sched_cfg = EntropySchedulerConfig(\n",
    "    use_scheduler=False\n",
    ")\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    total_updates=150,  # small number just for the demo\n",
    ")\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    eval_method=\"sample\",\n",
    "    n_steps=256\n",
    ")\n",
    "os.makedirs(agent_dir_path, exist_ok=True)\n",
    "\n",
    "logging_cfg = LoggingConfig(\n",
    "    current_update=0,\n",
    "    log_interval=2,\n",
    "    eval_interval=10,\n",
    "    html_log_path=None,          # keep it simple for the example\n",
    "    tensorboard_path=os.path.join(agent_dir_path, \"tensorboard_logs\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "saving_cfg = SavingConfig(\n",
    "    save_interval=25,\n",
    "    save_agent_path=agent_dir_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ce7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- networks ----------\n",
    "encoder = MLPEncoder(obs_shape=obs_shape, hidden_dim=32).to(device)\n",
    "\n",
    "actor_net = ActorNetwork(\n",
    "    \n",
    "    obs_embed_dim=encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_units=[32, 16],\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "critic_net = CriticNetwork(\n",
    "    obs_embed_dim=encoder.output_dim,\n",
    "    context_dim=context_dim,\n",
    "    hidden_units=[32, 16],\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "policy_net = PPOPolicyNetwork(\n",
    "    encoder=encoder,\n",
    "    action_head=actor_net,\n",
    "    value_head=critic_net,\n",
    ").to(device)\n",
    "\n",
    "policy_optimizer = Adam(policy_net.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483a467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- agent ----------\n",
    "agent: VectorizedPPOAgent = build_ppo_agent(\n",
    "    dims=dims,\n",
    "    ppo_cfg=ppo_cfg,\n",
    "    buf_cfg=buf_cfg,\n",
    "    entropy_sched_cfg=entropy_sched_cfg,\n",
    "    policy_net=policy_net,\n",
    "    policy_optimizer=policy_optimizer,\n",
    "    train_env=train_env,\n",
    "    eval_env=eval_env,\n",
    "    eval_cfg=eval_cfg,\n",
    "    logging_cfg=logging_cfg,\n",
    "    saving_cfg=saving_cfg,\n",
    "    device=device,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028c06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Upd   2 | step: 2048]\n",
      "| pi_loss: -0.0020  v_loss: +0.4564  ent: 1.082  ent_coef: 0.0100  kl: 0.0011  batches_used: 100.000%  expl_var: 0.10  clip: 0.00%\n",
      "| adv_mean: -6.4228  adv_std: 4.9774  rets_mean: -6.2440  rets_std: 4.9890\n",
      "| reward_sum: -1265.587  reward_pre_step: -0.61796\n",
      "[Grads] enc: 0.299  act: 0.125  critic: 0.343\n",
      "\n",
      "[Upd   4 | step: 4096]\n",
      "| pi_loss: -0.0030  v_loss: +0.3751  ent: 1.078  ent_coef: 0.0100  kl: 0.0003  batches_used: 100.000%  expl_var: 0.27  clip: 0.00%\n",
      "| adv_mean: -4.6439  adv_std: 4.0470  rets_mean: -4.4959  rets_std: 4.1836\n",
      "| reward_sum: -950.296  reward_pre_step: -0.46401\n",
      "[Grads] enc: 0.279  act: 0.155  critic: 0.335\n",
      "\n",
      "[Upd   6 | step: 6144]\n",
      "| pi_loss: -0.0050  v_loss: +0.2994  ent: 1.061  ent_coef: 0.0100  kl: 0.0028  batches_used: 100.000%  expl_var: 0.41  clip: 0.17%\n",
      "| adv_mean: -4.1427  adv_std: 3.4636  rets_mean: -4.0416  rets_std: 3.7431\n",
      "| reward_sum: -846.979  reward_pre_step: -0.41356\n",
      "[Grads] enc: 0.307  act: 0.154  critic: 0.313\n",
      "\n",
      "[Upd   8 | step: 8192]\n",
      "| pi_loss: -0.0032  v_loss: +0.2767  ent: 1.054  ent_coef: 0.0100  kl: 0.0010  batches_used: 100.000%  expl_var: 0.44  clip: 0.00%\n",
      "| adv_mean: -3.1488  adv_std: 2.4963  rets_mean: -3.0732  rets_std: 2.8576\n",
      "| reward_sum: -678.948  reward_pre_step: -0.33152\n",
      "[Grads] enc: 0.356  act: 0.14  critic: 0.264\n",
      "\n",
      "[Upd  10 | step: 10240]\n",
      "| pi_loss: -0.0064  v_loss: +0.2203  ent: 1.039  ent_coef: 0.0100  kl: 0.0023  batches_used: 100.000%  expl_var: 0.56  clip: 2.81%\n",
      "| adv_mean: -1.8100  adv_std: 1.5965  rets_mean: -1.6504  rets_std: 1.9750\n",
      "| reward_sum: -411.311  reward_pre_step: -0.20084\n",
      "[Grads] enc: 0.343  act: 0.159  critic: 0.281\n",
      "[Eval @ update 10] New best agent found! sum reward: -63.88, reward_per_step: -0.2495\n",
      "\n",
      "[Upd  12 | step: 12288]\n",
      "| pi_loss: -0.0088  v_loss: +0.1700  ent: 0.990  ent_coef: 0.0100  kl: 0.0028  batches_used: 100.000%  expl_var: 0.64  clip: 2.91%\n",
      "| adv_mean: -1.6423  adv_std: 1.5523  rets_mean: -1.6533  rets_std: 2.0890\n",
      "| reward_sum: -424.282  reward_pre_step: -0.20717\n",
      "[Grads] enc: 0.382  act: 0.176  critic: 0.227\n",
      "\n",
      "[Upd  14 | step: 14336]\n",
      "| pi_loss: -0.0089  v_loss: +0.2268  ent: 0.969  ent_coef: 0.0100  kl: 0.0024  batches_used: 100.000%  expl_var: 0.51  clip: 1.95%\n",
      "| adv_mean: -1.0404  adv_std: 0.6487  rets_mean: -0.9565  rets_std: 0.9745\n",
      "| reward_sum: -268.125  reward_pre_step: -0.13092\n",
      "[Grads] enc: 0.4  act: 0.176  critic: 0.21\n",
      "\n",
      "[Upd  16 | step: 16384]\n",
      "| pi_loss: -0.0102  v_loss: +0.1709  ent: 0.898  ent_coef: 0.0100  kl: 0.0087  batches_used: 100.000%  expl_var: 0.60  clip: 6.08%\n",
      "| adv_mean: -0.9075  adv_std: 0.5612  rets_mean: -0.8476  rets_std: 0.9704\n",
      "| reward_sum: -257.366  reward_pre_step: -0.12567\n",
      "[Grads] enc: 0.424  act: 0.176  critic: 0.159\n",
      "\n",
      "[Upd  18 | step: 18432]\n",
      "| pi_loss: -0.0048  v_loss: +0.1396  ent: 0.851  ent_coef: 0.0100  kl: 0.0035  batches_used: 100.000%  expl_var: 0.67  clip: 3.96%\n",
      "| adv_mean: -0.7686  adv_std: 0.4246  rets_mean: -0.7508  rets_std: 0.7966\n",
      "| reward_sum: -229.992  reward_pre_step: -0.11230\n",
      "[Grads] enc: 0.452  act: 0.152  critic: 0.123\n",
      "\n",
      "[Upd  20 | step: 20480]\n",
      "| pi_loss: -0.0047  v_loss: +0.2114  ent: 0.821  ent_coef: 0.0100  kl: 0.0071  batches_used: 100.000%  expl_var: 0.51  clip: 7.03%\n",
      "| adv_mean: -0.7260  adv_std: 0.3929  rets_mean: -0.6467  rets_std: 0.5888\n",
      "| reward_sum: -204.104  reward_pre_step: -0.09966\n",
      "[Grads] enc: 0.453  act: 0.132  critic: 0.138\n",
      "[Eval @ update 20] New best agent found! sum reward: -35.04, reward_per_step: -0.1369\n",
      "\n",
      "[Upd  22 | step: 22528]\n",
      "| pi_loss: -0.0020  v_loss: +0.1510  ent: 0.826  ent_coef: 0.0100  kl: 0.0030  batches_used: 100.000%  expl_var: 0.65  clip: 4.49%\n",
      "| adv_mean: -0.5935  adv_std: 0.4456  rets_mean: -0.7165  rets_std: 0.6949\n",
      "| reward_sum: -202.566  reward_pre_step: -0.09891\n",
      "[Grads] enc: 0.458  act: 0.126  critic: 0.132\n",
      "\n",
      "[Upd  24 | step: 24576]\n",
      "| pi_loss: -0.0041  v_loss: +0.1813  ent: 0.809  ent_coef: 0.0100  kl: 0.0047  batches_used: 100.000%  expl_var: 0.56  clip: 4.52%\n",
      "| adv_mean: -0.5855  adv_std: 0.3871  rets_mean: -0.5317  rets_std: 0.5591\n",
      "| reward_sum: -173.039  reward_pre_step: -0.08449\n",
      "[Grads] enc: 0.459  act: 0.138  critic: 0.119\n",
      "\n",
      "[Upd  26 | step: 26624]\n",
      "| pi_loss: -0.0036  v_loss: +0.1637  ent: 0.777  ent_coef: 0.0100  kl: 0.0048  batches_used: 100.000%  expl_var: 0.57  clip: 6.23%\n",
      "| adv_mean: -0.5835  adv_std: 0.4516  rets_mean: -0.5818  rets_std: 0.5448\n",
      "| reward_sum: -173.395  reward_pre_step: -0.08467\n",
      "[Grads] enc: 0.461  act: 0.133  critic: 0.103\n",
      "\n",
      "[Upd  28 | step: 28672]\n",
      "| pi_loss: +0.0001  v_loss: +0.1205  ent: 0.787  ent_coef: 0.0100  kl: 0.0042  batches_used: 100.000%  expl_var: 0.67  clip: 4.54%\n",
      "| adv_mean: -0.5120  adv_std: 0.3871  rets_mean: -0.5129  rets_std: 0.5565\n",
      "| reward_sum: -161.801  reward_pre_step: -0.07900\n",
      "[Grads] enc: 0.463  act: 0.134  critic: 0.104\n",
      "\n",
      "[Upd  30 | step: 30720]\n",
      "| pi_loss: -0.0024  v_loss: +0.1163  ent: 0.750  ent_coef: 0.0100  kl: 0.0046  batches_used: 100.000%  expl_var: 0.69  clip: 5.00%\n",
      "| adv_mean: -0.5220  adv_std: 0.4575  rets_mean: -0.5391  rets_std: 0.6129\n",
      "| reward_sum: -180.562  reward_pre_step: -0.08817\n",
      "[Grads] enc: 0.468  act: 0.124  critic: 0.101\n",
      "[Eval @ update 30] New best agent found! sum reward: -34.08, reward_per_step: -0.1331\n",
      "\n",
      "[Upd  32 | step: 32768]\n",
      "| pi_loss: +0.0005  v_loss: +0.1509  ent: 0.744  ent_coef: 0.0100  kl: 0.0012  batches_used: 100.000%  expl_var: 0.59  clip: 6.40%\n",
      "| adv_mean: -0.4814  adv_std: 0.3936  rets_mean: -0.4273  rets_std: 0.4876\n",
      "| reward_sum: -143.305  reward_pre_step: -0.06997\n",
      "[Grads] enc: 0.464  act: 0.124  critic: 0.103\n",
      "\n",
      "[Upd  34 | step: 34816]\n",
      "| pi_loss: +0.0002  v_loss: +0.1444  ent: 0.720  ent_coef: 0.0100  kl: 0.0031  batches_used: 100.000%  expl_var: 0.61  clip: 3.52%\n",
      "| adv_mean: -0.4286  adv_std: 0.4837  rets_mean: -0.4825  rets_std: 0.5444\n",
      "| reward_sum: -164.458  reward_pre_step: -0.08030\n",
      "[Grads] enc: 0.467  act: 0.105  critic: 0.116\n",
      "\n",
      "[Upd  36 | step: 36864]\n",
      "| pi_loss: +0.0004  v_loss: +0.0844  ent: 0.674  ent_coef: 0.0100  kl: 0.0037  batches_used: 100.000%  expl_var: 0.77  clip: 4.37%\n",
      "| adv_mean: -0.3429  adv_std: 0.5311  rets_mean: -0.4564  rets_std: 0.6285\n",
      "| reward_sum: -159.356  reward_pre_step: -0.07781\n",
      "[Grads] enc: 0.469  act: 0.112  critic: 0.106\n",
      "\n",
      "[Upd  38 | step: 38912]\n",
      "| pi_loss: -0.0022  v_loss: +0.1154  ent: 0.658  ent_coef: 0.0100  kl: 0.0035  batches_used: 100.000%  expl_var: 0.67  clip: 4.20%\n",
      "| adv_mean: -0.4287  adv_std: 0.3864  rets_mean: -0.3570  rets_std: 0.4876\n",
      "| reward_sum: -138.644  reward_pre_step: -0.06770\n",
      "[Grads] enc: 0.467  act: 0.12  critic: 0.106\n",
      "\n",
      "[Upd  40 | step: 40960]\n",
      "| pi_loss: +0.0000  v_loss: +0.0895  ent: 0.594  ent_coef: 0.0100  kl: 0.0098  batches_used: 100.000%  expl_var: 0.76  clip: 8.47%\n",
      "| adv_mean: -0.3155  adv_std: 0.5661  rets_mean: -0.4386  rets_std: 0.6116\n",
      "| reward_sum: -162.655  reward_pre_step: -0.07942\n",
      "[Grads] enc: 0.466  act: 0.11  critic: 0.114\n",
      "[Eval @ update 40] New best agent found! sum reward: -33.34, reward_per_step: -0.1302\n",
      "\n",
      "[Upd  42 | step: 43008]\n",
      "| pi_loss: -0.0027  v_loss: +0.1323  ent: 0.588  ent_coef: 0.0100  kl: 0.0077  batches_used: 100.000%  expl_var: 0.62  clip: 9.16%\n",
      "| adv_mean: -0.4160  adv_std: 0.3940  rets_mean: -0.3401  rets_std: 0.4553\n",
      "| reward_sum: -135.641  reward_pre_step: -0.06623\n",
      "[Grads] enc: 0.472  act: 0.112  critic: 0.0934\n",
      "\n",
      "[Upd  44 | step: 45056]\n",
      "| pi_loss: -0.0029  v_loss: +0.1185  ent: 0.559  ent_coef: 0.0100  kl: 0.0100  batches_used: 100.000%  expl_var: 0.65  clip: 10.52%\n",
      "| adv_mean: -0.3597  adv_std: 0.4572  rets_mean: -0.3808  rets_std: 0.5517\n",
      "| reward_sum: -137.196  reward_pre_step: -0.06699\n",
      "[Grads] enc: 0.474  act: 0.105  critic: 0.0951\n",
      "\n",
      "[Upd  46 | step: 47104]\n",
      "| pi_loss: -0.0016  v_loss: +0.1301  ent: 0.533  ent_coef: 0.0100  kl: 0.0050  batches_used: 100.000%  expl_var: 0.60  clip: 6.88%\n",
      "| adv_mean: -0.3345  adv_std: 0.4424  rets_mean: -0.2781  rets_std: 0.4678\n",
      "| reward_sum: -114.337  reward_pre_step: -0.05583\n",
      "[Grads] enc: 0.473  act: 0.11  critic: 0.0841\n",
      "\n",
      "[Upd  48 | step: 49152]\n",
      "| pi_loss: +0.0026  v_loss: +0.1035  ent: 0.524  ent_coef: 0.0100  kl: 0.0091  batches_used: 100.000%  expl_var: 0.70  clip: 8.94%\n",
      "| adv_mean: -0.2606  adv_std: 0.5813  rets_mean: -0.3676  rets_std: 0.5791\n",
      "| reward_sum: -142.239  reward_pre_step: -0.06945\n",
      "[Grads] enc: 0.475  act: 0.0917  critic: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(train_cfg\u001b[38;5;241m=\u001b[39mtrain_cfg)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppo_utils/vectorized_agent.py:513\u001b[0m, in \u001b[0;36mVectorizedPPOAgent.train\u001b[0;34m(self, train_cfg)\u001b[0m\n\u001b[1;32m    511\u001b[0m initial_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m initial_update \u001b[38;5;241m+\u001b[39m total_updates:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_one_update(update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppo_utils/vectorized_agent.py:528\u001b[0m, in \u001b[0;36mVectorizedPPOAgent.train_one_update\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# 1) Collect rollout & compute returns\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs) \n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# 2) PPO policy phase        \u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppo_utils/vectorized_agent.py:301\u001b[0m, in \u001b[0;36mVectorizedPPOAgent.collect_rollouts\u001b[0;34m(self, vec_env, obs)\u001b[0m\n\u001b[1;32m    297\u001b[0m next_obs, rewards, dones \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(actions_np)  \u001b[38;5;66;03m# likely numpy outputs\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# -------------- store in buffer -------------------\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# buffer will move things to its own device; we keep inputs on CPU here\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_buf\u001b[38;5;241m.\u001b[39madd_batch(\n\u001b[1;32m    302\u001b[0m     obs         \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mobs,            \u001b[38;5;66;03m# (n_envs, *obs_shape), torch\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     ctx         \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mctx,            \u001b[38;5;66;03m# (n_envs, *ctx_shape), torch\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     action_mask \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39maction_mask,    \u001b[38;5;66;03m# (n_envs, n_actions), torch\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     actions     \u001b[38;5;241m=\u001b[39m actions_cpu,        \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     logps       \u001b[38;5;241m=\u001b[39m logps\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),   \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     values      \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),  \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     rewards     \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;66;03m# (n_envs,), torch\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     dones       \u001b[38;5;241m=\u001b[39m dones\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),   \u001b[38;5;66;03m# (n_envs,), bool torch\u001b[39;00m\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    312\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m    313\u001b[0m last_dones \u001b[38;5;241m=\u001b[39m dones\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()    \u001b[38;5;66;03m# keep last dones for bootstrap masking\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_trading_sys/lib/python3.11/site-packages/rl_api/agents/ppo_utils/buffer.py:149\u001b[0m, in \u001b[0;36mRolloutBufferVec.add_batch\u001b[0;34m(self, obs, ctx, action_mask, actions, logps, values, rewards, dones)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m action_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n_envs,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask first dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != num_envs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    150\u001b[0m         obs         \u001b[38;5;241m=\u001b[39m obs[env_i],\n\u001b[1;32m    151\u001b[0m         context     \u001b[38;5;241m=\u001b[39m ctx[env_i],\n\u001b[1;32m    152\u001b[0m         action      \u001b[38;5;241m=\u001b[39m actions[env_i],\n\u001b[1;32m    153\u001b[0m         logp        \u001b[38;5;241m=\u001b[39m logps[env_i],\n\u001b[1;32m    154\u001b[0m         value       \u001b[38;5;241m=\u001b[39m values[env_i],\n\u001b[1;32m    155\u001b[0m         reward      \u001b[38;5;241m=\u001b[39m rewards[env_i],\n\u001b[1;32m    156\u001b[0m         done        \u001b[38;5;241m=\u001b[39m dones[env_i],\n\u001b[1;32m    157\u001b[0m         action_mask \u001b[38;5;241m=\u001b[39m action_mask[env_i],\n\u001b[1;32m    158\u001b[0m         env_id      \u001b[38;5;241m=\u001b[39m env_i,\n\u001b[1;32m    159\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(train_cfg=train_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_trading_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
